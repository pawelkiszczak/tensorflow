{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with TensorFlow Part 1: Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and fit a model using the same data used in main notebook but with MobileNetV2 architecture feature extraction [MobileNetV2](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5) from TensorFlow Hub. How does it perform compared to the other models trained in main notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 1 images in 04_exercise_dataset/\n",
      "There are 0 directories and 3 images in 04_exercise_dataset/model_ckpt\n",
      "There are 2 directories and 0 images in 04_exercise_dataset/10_food_classes_10_percent\n",
      "There are 10 directories and 0 images in 04_exercise_dataset/10_food_classes_10_percent/test\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/ice_cream\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/chicken_curry\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/steak\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/sushi\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/chicken_wings\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/grilled_salmon\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/hamburger\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/pizza\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/ramen\n",
      "There are 0 directories and 250 images in 04_exercise_dataset/10_food_classes_10_percent/test/fried_rice\n",
      "There are 10 directories and 0 images in 04_exercise_dataset/10_food_classes_10_percent/train\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/ice_cream\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/chicken_curry\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/steak\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/sushi\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/chicken_wings\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/grilled_salmon\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/hamburger\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/pizza\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/ramen\n",
      "There are 0 directories and 75 images in 04_exercise_dataset/10_food_classes_10_percent/train/fried_rice\n"
     ]
    }
   ],
   "source": [
    "# Let's check the structure and contains of each folder\n",
    "import os\n",
    "\n",
    "dataset_path = \"04_exercise_dataset/\"\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
    "\tprint(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: \n",
      "Found 750 images belonging to 10 classes.\n",
      "Test data: \n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setup the data inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Setup global variables\n",
    "IMAGE_SHAPE = (224, 224) # H x W x CC\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "\n",
    "train_dir = \"04_exercise_dataset/10_food_classes_10_percent/train/\"\n",
    "test_dir = \"04_exercise_dataset/10_food_classes_10_percent/test/\"\n",
    "\n",
    "# Rescale the images\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "# Run the pipelines\n",
    "print(\"Training images: \")\n",
    "train_data = train_datagen.flow_from_directory(directory=train_dir,\n",
    "                                               shuffle=True,\n",
    "                                               class_mode='categorical',\n",
    "                                               target_size=IMAGE_SHAPE,\n",
    "                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Test data: \")\n",
    "test_data = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                             shuffle=False,\n",
    "                                             class_mode='categorical',\n",
    "                                             target_size=IMAGE_SHAPE,\n",
    "                                             batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks for training\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define callbacks\n",
    "import datetime\n",
    "creation_time = datetime.datetime.now().strftime(\"%Y%m%d - %H%M%S\")\n",
    "\n",
    "checkpoint_dir = f\"04_exercise_dataset/model_ckpt/{creation_time}\"\n",
    "model_ckpt = ModelCheckpoint(filepath=checkpoint_dir,\n",
    "                             monitor='val_loss',\n",
    "                             save_weights_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', # maybe add start_from_epoch value for warm-up period?\n",
    "                           patience=2,\n",
    "                           verbose=1,\n",
    "                           restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for model to transfer\n",
    "mobilenet_v2 = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5\"\n",
    "\n",
    "# Import necessary tools\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " feature_extraction (KerasLa  (None, 1280)             2257984   \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                12810     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,270,794\n",
      "Trainable params: 12,810\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating model for URL\n",
    "feature_extraction_layer = hub.KerasLayer(handle=mobilenet_v2,\n",
    "                                          trainable=False,\n",
    "                                          name='feature_extraction',\n",
    "                                          input_shape=IMAGE_SHAPE+(3,))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extraction_layer,\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 23:22:44.090380: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - ETA: 0s - loss: 1.5298 - accuracy: 0.5067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 23:22:47.944853: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to 04_exercise_dataset/model_ckpt/20230707 - 232239\n",
      "94/94 [==============================] - 7s 54ms/step - loss: 1.5298 - accuracy: 0.5067 - val_loss: 1.5962 - val_accuracy: 0.4113\n",
      "Epoch 2/15\n",
      "93/94 [============================>.] - ETA: 0s - loss: 0.6757 - accuracy: 0.8181\n",
      "Epoch 2: saving model to 04_exercise_dataset/model_ckpt/20230707 - 232239\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.6792 - accuracy: 0.8173 - val_loss: 0.9944 - val_accuracy: 0.6411\n",
      "Epoch 3/15\n",
      "93/94 [============================>.] - ETA: 0s - loss: 0.4744 - accuracy: 0.8774\n",
      "Epoch 3: saving model to 04_exercise_dataset/model_ckpt/20230707 - 232239\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.4727 - accuracy: 0.8787 - val_loss: 0.8279 - val_accuracy: 0.7177\n",
      "Epoch 4/15\n",
      "93/94 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.9272\n",
      "Epoch 4: saving model to 04_exercise_dataset/model_ckpt/20230707 - 232239\n",
      "94/94 [==============================] - 4s 44ms/step - loss: 0.3385 - accuracy: 0.9280 - val_loss: 0.8436 - val_accuracy: 0.7218\n",
      "Epoch 5/15\n",
      "93/94 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9569Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 5: saving model to 04_exercise_dataset/model_ckpt/20230707 - 232239\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.2602 - accuracy: 0.9573 - val_loss: 0.8471 - val_accuracy: 0.6976\n",
      "Epoch 5: early stopping\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.6206 - accuracy: 0.8012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6206080913543701, 0.8011999726295471]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history_model = model.fit(train_data,\n",
    "                          epochs=15,\n",
    "                          steps_per_epoch=len(train_data),\n",
    "                          validation_data=test_data,\n",
    "                          validation_steps=int(0.1*len(test_data)),\n",
    "                          callbacks=[early_stop, model_ckpt])\n",
    "\n",
    "# Evaluate\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on **MobileNetV2** was able to achieve 0.599 loss and 80.1% accuracy on the test set in 6 epochs. Comparing it to the previous models (from the main notebook):\n",
    "* **ResNET V50** - loss: 0.651, acccuracy: 77.6% in 5 epochs\n",
    "* **EfficientNetB0** - loss: 0.406, accuracy: 87.9% in 15 epochs\n",
    "\n",
    "Given the mentioned results, MobileNetV2 is comparable to ResNET V50 architecture having approx. 10 times less parameters (2.27M vs 23.5M) and taking less time per epoch averaging at 5 seconds versus 18 seconds for the latter.\n",
    "\n",
    "EfficientNetB0 however seems to be a clear winner with close to 10% better accuracy, 50% lower loss metric at approx. 4.04M parameters and epoch time averaging at 15 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model to classify images of two different thing you've taken photos of.\n",
    "* You can usse any feature extraction layer from TensorFlow Hub you like for this.\n",
    "* You should aim to have at least 10 images of each class.\n",
    "* Compare its performance to previous models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
